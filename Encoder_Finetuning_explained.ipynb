{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning of Feature encoder\n",
    "\n",
    "A very common way of applying deep learning techniques in digital pathology is Multiple Instance Learning (MIL) \n",
    "The giga pixel image is cropped into a set of equally sized non overlapping tiles. The tiles are encoded with a pretrained feature extractor and then aggregated within in a second step by a trainable model to solve a specific task. \n",
    "Tipically used feature encoders were pretrained either on Imagenet21k or even on larger histological datasets. The architectures of such models can reach from smaller ones like the Resnet18  up to Swim transformer based transformer architectures. \n",
    "Since pre training larger models from scratch requires huge amounts of ressources, the focus of this model is fine tuning a pretrained model. \n",
    "\n",
    "**The following experiments attempt to explore whether existing feature encoders can be fine-tuned for better survival analysis.**\n",
    "\n",
    "\n",
    "The aim is to carry out the following experiments:\n",
    "\n",
    "1. Resnet18: train from scratch/fine tune on Survival Analysis\n",
    "2. ViT Tiny: train from scratch/fine tune on Survival Analysis (+Multimodality)\n",
    "3. Vit Tiny MAE: fine tune on Survival Analysis \n",
    "4. Vit Tiny MAE: fine tune on Survival Analysis in a supMAE fashion\n",
    "\n",
    "To aquire those experiments, the following subtasks are needed: \n",
    "1. Create a custom dataset(from zip) that statifies on a patient level into a train/test split.\n",
    "2. Create models,find checkpoints,load parameters such that DDP is applicable\n",
    "3. Create a training function which allows partially freezing weights, finetune, train from checkpoint.\n",
    "4. Create an encoding pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "\n",
    "The Data consists of ~1000 patients. Each patient has exactly one genetic feature vector and can have multiple sets(can have multiple slides) of tile-sets.\n",
    "\n",
    "Idea: \n",
    "\n",
    "The dataloader receives a dataframe which contains meta data and genetic data (stratified by train/test split on a patient level)\n",
    "and further a path to the tiles. From this path, an os.walk is done to create a second dataframe which contains the file path and the slide_id.\n",
    "the dataframe will be adapted to contain the tile_path, meta data and the index of the respecitve row within the genomic tensor\n",
    "\n",
    "1. Import Dataframe, create Gen tensor, metadataframe\n",
    "2. os.walk on tilepath to create tile dataframe,\n",
    "3.  add mapping for slide_id idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.)\n",
    "from utils.Aggregation_Utils import *\n",
    "df_path = ...\n",
    "df_train,df_test,df_val = prepare_csv(df_path=\"/work4/seibel/PORPOISE/datasets_csv/tcga_brca_all_clean.csv.zip\",split=\"traintestval\",n_bins=4,save = False,frac_train=0.7,frac_val=0.1)\n",
    "df_train = df_train[df_train[\"traintest\"]==0] # if train \n",
    "\n",
    "\n",
    "\n",
    "genomics_tensor = torch.Tensor(df_train[df_train.keys()[11:]].to_numpy()).to(torch.float32)\n",
    "df_meta = df_train[[\"slide_id\",\"survival_months_discretized\",\"censorship\",\"survival_months\"]]\n",
    "diction = dict([(name,idx) for idx,name in enumerate(df_meta[\"slide_id\"]) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AED-F1E3C52A776F/TCGA-AN-A0XP-01Z-00-DX1_(15207,21291).jpg\n",
      "TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AED-F1E3C52A776F.svs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tilepath</th>\n",
       "      <th>slide_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            tilepath  \\\n",
       "0  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "1  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "2  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "3  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "4  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "\n",
       "                                            slide_id  \n",
       "0  TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...  \n",
       "1  TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...  \n",
       "2  TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...  \n",
       "3  TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...  \n",
       "4  TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.)\n",
    "import os\n",
    "import pandas as pd\n",
    "tile_path = \"/globalwork/seibel/TCGA-BRCA-TILES-NORM/\"\n",
    "ext = \"jpg\"\n",
    "file_list = []\n",
    "root_list = []\n",
    "for root, dirs, files in os.walk(tile_path, topdown=False):\n",
    "    for name in files:\n",
    "        file_list.append(os.path.join(root, name))\n",
    "        root_list.append(root.split(\"/\")[-1]+\".svs\")\n",
    "\n",
    "df_tiles = pd.DataFrame({\"tilepath\":file_list,\"slide_id\":root_list},)\n",
    "df_tiles = df_tiles[df_tiles[\"tilepath\"].str.endswith(ext)] # Avoid having other files than .<ext> files in Dataframe\n",
    "\n",
    "\n",
    "print(df_tiles.tilepath.iloc[0])\n",
    "print(df_tiles.slide_id.iloc[0])\n",
    "df_tiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## safe dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title = \"df-TCGA-BRCA-TIILES-NORM.csv\"\n",
    "#df_tiles.to_csv(title,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6488/4175619591.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_tiles.slideid_idx = df_tiles.slideid_idx.astype(int)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tilepath</th>\n",
       "      <th>slide_id</th>\n",
       "      <th>slideid_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943700</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943701</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943702</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943703</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943704</th>\n",
       "      <td>/globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...</td>\n",
       "      <td>TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1852318 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tilepath  \\\n",
       "0        /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "1        /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "2        /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "3        /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "4        /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "...                                                    ...   \n",
       "2943700  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "2943701  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "2943702  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "2943703  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "2943704  /globalwork/seibel/TCGA-BRCA-TILES-NORM/TCGA-A...   \n",
       "\n",
       "                                                  slide_id  slideid_idx  \n",
       "0        TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...          289  \n",
       "1        TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...          289  \n",
       "2        TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...          289  \n",
       "3        TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...          289  \n",
       "4        TCGA-AN-A0XP-01Z-00-DX1.A4EE3970-5C1F-482E-9AE...          289  \n",
       "...                                                    ...          ...  \n",
       "2943700  TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...           74  \n",
       "2943701  TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...           74  \n",
       "2943702  TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...           74  \n",
       "2943703  TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...           74  \n",
       "2943704  TCGA-A2-A0YD-01Z-00-DX1.B81FF541-F154-4C49-950...           74  \n",
       "\n",
       "[1852318 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.)\n",
    "df_tiles.insert(2,\"slideid_idx\",df_tiles[\"slide_id\"].map(diction))\n",
    "df_tiles = df_tiles.dropna()\n",
    "df_tiles.slideid_idx = df_tiles.slideid_idx.astype(int)\n",
    "df_tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Custom Dataset\n",
    "It is better to create the dataframe once instead of including the os-walk within the dataloader! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8392, 0.7569, 0.8000,  ..., 0.9373, 0.9373, 0.9412],\n",
       "          [0.7843, 0.7412, 0.7765,  ..., 0.9373, 0.9373, 0.9412],\n",
       "          [0.7373, 0.7412, 0.7608,  ..., 0.9373, 0.9373, 0.9412],\n",
       "          ...,\n",
       "          [0.9255, 0.9294, 0.9176,  ..., 0.9451, 0.9529, 0.9412],\n",
       "          [0.9373, 0.9294, 0.9059,  ..., 0.9412, 0.9569, 0.9412],\n",
       "          [0.9333, 0.9373, 0.9333,  ..., 0.9373, 0.9569, 0.9412]],\n",
       " \n",
       "         [[0.7255, 0.6549, 0.7137,  ..., 0.9373, 0.9373, 0.9412],\n",
       "          [0.6706, 0.6392, 0.6902,  ..., 0.9373, 0.9373, 0.9412],\n",
       "          [0.6275, 0.6392, 0.6745,  ..., 0.9373, 0.9373, 0.9412],\n",
       "          ...,\n",
       "          [0.9294, 0.9333, 0.9216,  ..., 0.9373, 0.9451, 0.9333],\n",
       "          [0.9529, 0.9451, 0.9216,  ..., 0.9333, 0.9490, 0.9333],\n",
       "          [0.9490, 0.9529, 0.9490,  ..., 0.9294, 0.9490, 0.9333]],\n",
       " \n",
       "         [[0.8353, 0.7529, 0.7961,  ..., 0.9451, 0.9451, 0.9490],\n",
       "          [0.7804, 0.7373, 0.7725,  ..., 0.9451, 0.9451, 0.9490],\n",
       "          [0.7255, 0.7373, 0.7569,  ..., 0.9451, 0.9451, 0.9490],\n",
       "          ...,\n",
       "          [0.9451, 0.9490, 0.9373,  ..., 0.9490, 0.9490, 0.9373],\n",
       "          [0.9569, 0.9490, 0.9255,  ..., 0.9451, 0.9529, 0.9373],\n",
       "          [0.9529, 0.9569, 0.9451,  ..., 0.9412, 0.9529, 0.9373]]]),\n",
       " tensor([ 0.3896,  0.3837,  0.3837,  ...,  0.8423, -0.6116, -0.5707]),\n",
       " tensor(0),\n",
       " tensor(0),\n",
       " tensor(20))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "from utils.Aggregation_Utils import *\n",
    "import pandas as pd\n",
    "import os \n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "class TileDataset(Dataset):\n",
    "    def __init__(self,df_path,tile_path,ext,trainmode,transform):\n",
    "        \"\"\"Custom Dataset for Feature Extractor Finetuning for Survival Analysis \n",
    "\n",
    "        Args:\n",
    "            df_path (str): Path to Dataframe which contains meta data and genomic data \n",
    "            tilepath (str): path to folder which contains subfolders with tiles(subfolder names must ne slide id)\n",
    "            ext (str): file extension of tiles(eg jpg or png)\n",
    "            trainmode (Bool): To generate train set or test set \n",
    "        \"\"\"\n",
    "        super(TileDataset,self).__init__()\n",
    "        #Genomic Tensor and Meta Dataframe\n",
    "        df = pd.read_csv(df_path) \n",
    "\n",
    "        assert trainmode in [\"train\",\"test\",\"val\"], \"Dataset mode not known\"\n",
    "        df[df[\"traintest\"]==(0 if trainmode==\"train\" else 1 if trainmode==\"test\" else 2)]\n",
    "        \n",
    "            \n",
    "        self.genomics_tensor = torch.Tensor(df[df.keys()[11:]].to_numpy()).to(torch.float32)\n",
    "        self.df_meta = df[[\"slide_id\",\"survival_months_discretized\",\"censorship\",\"survival_months\"]]\n",
    "        \n",
    "        # Tile Data Frame\n",
    "        file_list = []\n",
    "        root_list = []\n",
    "        for root, dirs, files in os.walk(tile_path, topdown=False):\n",
    "            for name in files:\n",
    "                file_list.append(os.path.join(root, name))\n",
    "                root_list.append(root.split(\"/\")[-1]+\".svs\")\n",
    "\n",
    "\n",
    "        df_tiles = pd.DataFrame({\"tilepath\":file_list,\"slide_id\":root_list},)\n",
    "        df_tiles = df_tiles[df_tiles[\"tilepath\"].str.endswith(ext)]\n",
    "        \n",
    "        # add slide_id to index mapping\n",
    "        diction= dict([(name,idx) for idx,name in enumerate(self.df_meta[\"slide_id\"]) ]) \n",
    "        df_tiles.insert(2,\"slideid_idx\",df_tiles[\"slide_id\"].map(diction))\n",
    "        df_tiles = df_tiles.dropna()\n",
    "        df_tiles.slideid_idx = df_tiles.slideid_idx.astype(int)\n",
    "        self.df_tiles = df_tiles\n",
    "        \n",
    "        # TODO transforms \n",
    "        self.transforms = transform\n",
    "    def __len__(self):\n",
    "        return len(self.df_tiles)\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        tile_path,_,slide_idx = self.df_tiles.iloc[idx]\n",
    "        tile = Image.open(tile_path)\n",
    "        tile = self.transforms(tile)\n",
    "        \n",
    "        label = torch.tensor(self.df_meta.iloc[slide_idx, 1]).type(torch.int64)\n",
    "        censorship = torch.tensor(self.df_meta.iloc[slide_idx, 2]).type(torch.int64)\n",
    "        label_cont = torch.tensor(self.df_meta.iloc[slide_idx,3]).type(torch.int64)\n",
    "        return tile, self.genomics_tensor[slide_idx], censorship, label,label_cont\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "df_path_train = \"/work4/seibel/PORPOISE/datasets_csv/tcga_brca__4bins_trainsplit.csv\"\n",
    "df_path_test = \"/work4/seibel/PORPOISE/datasets_csv/tcga_brca__4bins_testsplit.csv\"\n",
    "\n",
    "tilepath = \"/work4/seibel/data/TCGA-BRCA-TILES/\"\n",
    "ext = \"jpg\"\n",
    "\n",
    "trainmode=\"train\"\n",
    "\n",
    "\n",
    "DS = TileDataset(df_path_train,tilepath,ext,trainmode,transform=transforms.ToTensor())\n",
    "DS.__getitem__(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.Tile_Dataset import TileDataset\n",
    "from torchvision import transforms\n",
    "import os \n",
    "df_path_train =\"/nodes/bevog/work4/seibel/PORPOISE/datasets_csv/tcga_brca__4bins_trainsplit.csv\"\n",
    "assert os.path.exists(df_path_train)\n",
    "tile_path =\"/nodes/bevog/work4/seibel/data/TCGA-BRCA-TILES/\"\n",
    "ext = \"jpg\"\n",
    "batch_size = 32\n",
    "transform_train=transforms.PILToTensor()\n",
    "train_set = TileDataset(df_path=df_path_train,tile_path=tile_path,ext=ext,trainmode = \"train\",transform=transform_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss \n",
    "The loss can be used from the previous colab notebook but has to be adapted to fully run on gpu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seibel/.pyenv/versions/3.10.12/envs/env/lib/python3.10/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.8186)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.Aggregation_Utils import Survival_Loss\n",
    "import torch\n",
    "B = 10\n",
    "nbins  = 4 \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "#hist_tile,gen, censorship, label,label_cont = DS.__getitem__(8)\n",
    "\n",
    "criterion = Survival_Loss(0.2)\n",
    "prediction_logits = torch.rand(B,nbins,device=device)\n",
    "c  = torch.randint(0,2,size=(B,1),device=device)\n",
    "l = torch.randint(0,nbins,size=(B,),device=device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "criterion(prediction_logits,c,l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "channels = 12\n",
    "model = nn.Sequential(nn.Conv2d(3, channels, kernel_size=(3, 3), padding='same', bias=True),\n",
    "                          nn.AdaptiveAvgPool2d(1),\n",
    "                          nn.Flatten(1),\n",
    "                          nn.Linear(channels,1),\n",
    "                          nn.Flatten(0))\n",
    "\n",
    "\n",
    "\n",
    "x = torch.rand((5,3,16,16))\n",
    "model(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "f = \"./encoder_configs/base.yaml\"\n",
    "os.path.exists(f)\n",
    "with open(f, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "        \n",
    "print(config[\"train_settings\"][\"checkpoint_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF1 = ['case_id', 'slide_id', 'site', 'traintest', 'metas','gendata']\n",
    "DF2 = ['TILEPATH','SLIDE_ID']\n",
    "#get one tensor for gendata, one df with tilepath, metadata, tensoridx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"vali\"\n",
    "assert mode in [\"train\",\"test\",\"val\"], \"Dataset mode not known\"\n",
    "df_train.survival_months_discretized[df_train.survival_months_discretized==( 0 if mode==\"train\" else 1 if mode==\"test\" else 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = 2\n",
    "train_len = 3\n",
    "diction = dict([(name,0) if idx<train_len else (name,1) if idx<train_len+test_len  else (name,2) for idx,name in enumerate([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"])])\n",
    "diction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "bins = 4\n",
    "B=12\n",
    "out_all =[]\n",
    "c_all =  []\n",
    "l_all = []\n",
    "\n",
    "for i in range(9):\n",
    "    out = torch.rand((B,bins))\n",
    "    l = torch.randint(0,bins,size=(B,))\n",
    "    c = torch.randint(0,2,size=(B,))\n",
    "    \n",
    "    out_all.append(out)\n",
    "    l_all.append(l)\n",
    "    c_all.append(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([108, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(out_all,dim=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_censored\n",
    "h = nn.Sigmoid()(torch.cat(out_all,dim=0))\n",
    "S = torch.cumprod(1-h,dim = -1)\n",
    "risk = -S.sum(dim=1) \n",
    "notc = (1-torch.cat(c_all,dim=0)).numpy().astype(bool)\n",
    "c_index = concordance_index_censored(notc, torch.cat(l_all,dim=0),risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "class MAE(nn.Module):\n",
    "    def __init__(self,multimodal,supervised_surv):\n",
    "        super(MAE,self).__init__()\n",
    "        self.encoder = nn.Identity()\n",
    "        self.decoder = nn.Identity()\n",
    "        self.enc_emb = ...\n",
    "        self.dec_emb = ...\n",
    "        self.enc_pos_emb = ...\n",
    "        self.dec_pos_emb = ...\n",
    "        self.y_encoder = ...    \n",
    "        self.masktoken = ...\n",
    "    def forward(self,x,y):\n",
    "        x_seq =  img2seq(x)\n",
    "        unmasked,masked,unmasked_idx,masked_idx = self.masking(x)\n",
    "        encoded = self.encoder(torch.concat([self.enc_emb(unmasked)+self.enc_pos_emb(unmasked_idx),self.y_encoder(y)],dim=1))\n",
    "        unmasked_enc,y_enc = torch.split(encoded)# find correct way of splitting\n",
    "        decoded = torch.cat([self.dec_emb(unmasked_enc)+self.dec_pos_emb(unmasked_idx),self.dec_pos_emb(masked_idx)+ maskedtoken[None:,:]],dim=1)\n",
    "        _,decoded_masked = torch.split(decoded)#split based on length of mask\n",
    "        \n",
    "        if self.supervised_surv:\n",
    "            surv_in = torch.stack([y_enc,torch.mean(unmasked_enc)])\n",
    "            \n",
    "            \n",
    "        \n",
    "        return masked, decoded_masked\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    def img2seq():\n",
    "        ...\n",
    "    def seq2img():\n",
    "        ...\n",
    "    def masking():\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seibel/.pyenv/versions/3.10.12/envs/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!pip install timm\n",
    "from models.mae_models.models_mae_modified import mae_vit_tiny_patch16\n",
    "model = mae_vit_tiny_patch16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "B,C,H,W = 20,3,224,224\n",
    "imgs =  torch.rand((B,C,H,W))\n",
    "mask_ratio = 0.75\n",
    "y = torch.rand((B,1,192))\n",
    "ids_shuffle=None\n",
    "surv = True\n",
    "\n",
    "latent, mask, ids_restore, ids_shuffle = model.forward_encoder(imgs, mask_ratio,y, ids_shuffle)\n",
    "latent_hist,latent_gen = torch.split(latent,split_size_or_sections=[latent.size(1)-1,1],dim=1)\n",
    "pred = model.forward_decoder(latent_hist, ids_restore)  # [N, L, p*p*3]\n",
    "lossMAE = model.forward_loss(imgs, pred, mask)\n",
    "\n",
    "if surv:\n",
    "    surv_in = torch.cat((torch.mean(latent_hist,dim=1),latent_gen.squeeze(1)),dim=1)\n",
    "    survivalhead\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path = \"/work4/seibel/data/mae_tiny_400e.pth.tar\"\n",
    "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "state_dict = {k.replace(\"module.model.\", \"\"): v for k, v in ckpt[\"model\"].items()}\n",
    "model.load_state_dict(state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

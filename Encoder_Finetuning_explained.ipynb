{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning of Feature encoder\n",
    "\n",
    "A very common way of applying deep learning techniques in digital pathology is Multiple Instance Learning (MIL) \n",
    "The giga pixel image is cropped into a set of equally sized non overlapping tiles. The tiles are encoded with a pretrained feature extractor and then aggregated within in a second step by a trainable model to solve a specific task. \n",
    "Tipically used feature encoders were pretrained either on Imagenet21k or even on larger histological datasets. The architectures of such models can reach from smaller ones like the Resnet18  up to Swim transformer based transformer architectures. \n",
    "Since pre training larger models from scratch requires huge amounts of ressources, the focus of this model is fine tuning a pretrained model. \n",
    "\n",
    "**The following experiments attempt to explore whether existing feature encoders can be fine-tuned for better survival analysis.**\n",
    "\n",
    "\n",
    "The aim is to carry out the following experiments:\n",
    "\n",
    "1. Resnet18: train from scratch/fine tune on Survival Analysis\n",
    "2. ViT Tiny: train from scratch/fine tune on Survival Analysis (+Multimodality)\n",
    "3. Vit Tiny MAE: fine tune on Survival Analysis \n",
    "4. Vit Tiny MAE: fine tune on Survival Analysis in a supMAE fashion\n",
    "\n",
    "To aquire those experiments, the following subtasks are needed: \n",
    "1. Create a custom dataset(from zip) that statifies on a patient level into a train/test split.\n",
    "2. Create models,find checkpoints,load parameters such that DDP is applicable\n",
    "3. Create a training function which allows partially freezing weights, finetune, train from checkpoint.\n",
    "4. Create an encoding pipeline. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue from Checkpointpath: \n",
      " 4\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 4 \n",
    "print(f\"Continue from Checkpointpath: \\n {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "f = \"./encoder_configs/base.yaml\"\n",
    "os.path.exists(f)\n",
    "with open(f, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "        \n",
    "config[\"machine_setting\"][\"local_rank\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

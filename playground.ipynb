{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"data/tcga_brca_all_clean.csv.zip\"\n",
    "\n",
    "#TODO slide_id -> file path "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing genomic data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 4\n",
    "df = pd.read_csv(f,compression='zip')\n",
    "\n",
    "df_uncensored = (df[df[\"censorship\"]==0]).drop_duplicates([\"case_id\"])\n",
    "_,bins = pd.qcut(df_uncensored['survival_months'],q = n_bins,retbins=True)  # distribute censored survival months into quartiles\n",
    "\n",
    "# adapt bins \n",
    "bins[0] = 0 \n",
    "bins[-1] = np.inf\n",
    "#\n",
    "labels=[i for i in range(n_bins)]\n",
    "labels\n",
    "\n",
    "df.insert(6,\"survival_months_discretized\",  pd.cut(df[\"survival_months\"],\n",
    "                                                               bins=bins, \n",
    "                                                               labels=labels)) # insert binned survival momnth \n",
    "k = 7\n",
    "df.insert(3,\"kfold\",df.index%k) # insert kfold \n",
    "df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale genomic data first and store in DF again\n",
    "then add labels to stratify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo add new column k-th-fold that contains an int in arange(k) where k is a cluster \n",
    "groundtruth = df[\"survival_months_discretized\"]\n",
    "censorship = df[\"censorship\"]\n",
    "genomics = df[df.keys()[11:]]\n",
    "genomics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_genomics = scaler.fit_transform(genomics)\n",
    "#scaler.inverse_transform(scaled_genomics)\n",
    "df_scaled = df.copy()\n",
    "df_scaled[df.keys()[11:]] = scaled_genomics\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  This leads to the full function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean2trainable(df_path,k,n_bins=4,savename = None):\n",
    "    df = pd.read_csv(df_path,compression='zip')\n",
    "\n",
    "    # get time bins \n",
    "    df_uncensored = (df[df[\"censorship\"]==0]).drop_duplicates([\"case_id\"])\n",
    "    _,bins = pd.qcut(df_uncensored['survival_months'],q = n_bins,retbins=True)  # distribute censored survival months into quartiles\n",
    "\n",
    "    # adapt time bins \n",
    "    bins[0] = 0 \n",
    "    bins[-1] = np.inf\n",
    "    # bin name = index \n",
    "    labels = [i for i in range(n_bins)]\n",
    "    df.insert(6,\"survival_months_discretized\",  pd.cut(df[\"survival_months\"],\n",
    "                                                               bins=bins, \n",
    "                                                               labels=labels)) # insert binned survival month \n",
    "    df.insert(3,\"kfold\",df.index%k) # insert kfold \n",
    "\n",
    "    genomics = df[df.keys()[11:]]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_genomics = scaler.fit_transform(genomics)\n",
    "    df[df.keys()[11:]] = scaled_genomics\n",
    "\n",
    "    if savename is not None:\n",
    "        df.to_csv(savename,index=False)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"data/tcga_brca_all_clean.csv.zip\"\n",
    "new_df = clean2trainable(f,5,4,savename = \"./data/tcga_brca_trainable.csv\")\n",
    "#new_df = clean2trainable(f,7,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_new = \"data/tcga_brca_trainable.csv\"\n",
    "df = pd.read_csv(f_new)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.keys()[11:]].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Fold crossvalidation\n",
    "1. Cluster dataset into k groups (shuffle first(seeded) then stratify by patient,split into equal chunks)\n",
    "2. use one group for testing and the others for training\n",
    "3. rotate such that each group is once used for testing\n",
    "4. use distribution of testing results to estimate real value \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7 \n",
    "a = np.arange(k) # df k-th fold \n",
    "for i in range(k):  \n",
    "    a = (a+1)%k\n",
    "    print(f\"train on {a[:-1]} test on {a[-1]}, store results in foldername: experiment1_fold{a[-1]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"col1\": np.arange(100),\"col2\":np.random.rand(100)})\n",
    "df\n",
    "df[\"col1\"] = df[\"col1\"].apply(lambda x: f\"thisis{x}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"data/tcga_brca_all_clean.csv.zip\"\n",
    "f = f.replace(\"all_clean.csv.zip\",\"trainable.csv\")\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "idx = torch.randint(low=0,high=4,size=(100,))\n",
    "values = torch.zeros(size=(100,4))\n",
    "source = torch.ones(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.scatter_(dim=1,index=idx,src=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth = torch.zeros(size=(10,4))\n",
    "a,b = groundtruth.size()\n",
    "labels = torch.randint(low=0,high=b,size=(1,a))\n",
    "groundtruth[torch.arange(a),labels]=1\n",
    "groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "acc = Accuracy(\"multiclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randint(low=0,high=2,size=(100,))\n",
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "c = torch.randint(0,2,size=(100,))\n",
    "l = torch.randint(0,4,size=(100,))\n",
    "out = nn.Sigmoid()(torch.rand(size=(100,4)))\n",
    "c_index = concordance_index_censored(c,l,out)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "class Classifier_Head(nn.Module):\n",
    "    def __init__(self,outsize,d_hidden=256,t_bins=4):\n",
    "        super(Classifier_Head,self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(outsize,d_hidden)\n",
    "        torch.nn.init.kaiming_normal_(self.linear1.weight)\n",
    "        self.activ1 = nn.ReLU()\n",
    "        self.linear2  = nn.Linear(d_hidden,d_hidden)\n",
    "        torch.nn.init.kaiming_normal_(self.linear2.weight)\n",
    "        self.activ2 = nn.ReLU()\n",
    "        self.fc = nn.Linear(d_hidden,t_bins) # TODO test add layer\n",
    "    def forward(self,x):\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.activ1(self.linear1(x))\n",
    "        x = self.activ2(self.linear2(x))\n",
    "        return self.fc(x)\n",
    "\n",
    "class Attention_surv(nn.Module):\n",
    "    def __init__(self,hist_dim,bins):\n",
    "        super(Attention_surv,self).__init__()\n",
    "        self.mhsa = nn.MultiheadAttention(hist_dim,num_heads=4,dropout=0.3,batch_first=True)\n",
    "        self.head = Classifier_Head(hist_dim,t_bins = bins)\n",
    "    def forward(self,hist):\n",
    "        attn_output, attn_output_weights = self.mhsa(hist,hist,hist)\n",
    "        attn_output = attn_output.mean(dim=1)\n",
    "        return self.head(attn_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multi_modal_ds import HistGen_Dataset\n",
    "import pandas as pd\n",
    "from utils import Survival_Loss\n",
    "alpha = 0.25\n",
    "bins = 4\n",
    "lr = 2e-4\n",
    "l1_lambda = 1e-5\n",
    "\n",
    "f = f\"/nodes/bevog/work4/seibel/data/tcga_brca_trainable{bins}.csv\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data_path = \"/nodes/bevog/work4/seibel/data/TCGA-BRCA-DX-features/tcga_brca_20x_features/pt_files\" # folderpath for h5y fiels which contain the WSI feat vecs \n",
    "batchsize = 1  # due to different size of bags \n",
    "\n",
    "df = pd.read_csv(f)\n",
    "train_ds = HistGen_Dataset(df,data_path = data_path,train=True)\n",
    "test_ds = HistGen_Dataset(df,data_path = data_path,train=False)\n",
    "training_dataloader = torch.utils.data.DataLoader( train_ds,batch_size=batchsize)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_ds,batch_size=batchsize)\n",
    "\n",
    "\n",
    "model = Attention_surv(hist_dim=2048,bins=bins).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr,betas=[0.9,0.999],weight_decay=1e-5,)\n",
    "criterion = Survival_Loss(alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,(histo,gen,c,l,_) in enumerate(training_dataloader):\n",
    "    histo = histo.to(device)\n",
    "    out = model(histo)\n",
    "    out = out.cpu()\n",
    "    #weights = model.mhsa.parameters()\n",
    "    loss = criterion(out,c,l) #+ l1_lambda * torch.norm(weights.cpu(),1)\n",
    "    loss.backward()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KaplanMeier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "bins = 4\n",
    "f = f\"/nodes/bevog/work4/seibel/data/tcga_brca_trainable{bins}.csv\"\n",
    "df = pd.read_csv(f)\n",
    "df = df[[\"slide_id\",\"survival_months_discretized\",\"censorship\",\"survival_months\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(kaplan_meier_estimator)\n",
    "event = np.asarray(1-df[\"censorship\"]).astype(bool)\n",
    "time_exit = np.asarray(df[\"survival_months\"])\n",
    "\n",
    "\n",
    "x_full, y_full = kaplan_meier_estimator(event, time_exit)\n",
    "plt.step(x_full, y_full, where=\"post\")\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import AttMil_Survival\n",
    "from multi_modal_ds import HistGen_Dataset\n",
    "import torch \n",
    "from torch import nn \n",
    "from utils import c_index\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#load model \n",
    "d_hist = 2048\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "bins = 4\n",
    "model = AttMil_Survival(d_hist,bins,device ).to(device)\n",
    "#load weights \n",
    "f_weights = \"/work4/seibel/results/alpha0.5hist/AttMil_Survival_nll-alpha0.5-fold5-l1_lambda1e-07.pth\"\n",
    "weights = torch.load(f_weights)['model']\n",
    "model.load_state_dict(weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f =  f\"/nodes/bevog/work4/seibel/data/tcga_brca_trainable{bins}.csv\"\n",
    "df = pd.read_csv(f)\n",
    "data_path = \"/nodes/bevog/work4/seibel/data/TCGA-BRCA-DX-features/tcga_brca_20x_features/pt_files\" \n",
    "test_ds = HistGen_Dataset(df,data_path,train=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_ds,batch_size=1)\n",
    "\n",
    "out_all_val =torch.empty(size=(len(test_dataloader),bins),device='cpu')        \n",
    "l_all_val = torch.empty(size=(len(test_dataloader),),device='cpu').to(torch.int16)\n",
    "l_cont_all_val = torch.empty(size=(len(test_dataloader),),device='cpu').to(torch.int16)\n",
    "c_all_val = torch.empty(size=(len(test_dataloader),),device='cpu').to(torch.int16)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for  idx,(histo,gen,c,l,l_cont) in enumerate(test_dataloader):\n",
    "        x = histo.to(device)\n",
    "        out = model(x)\n",
    "        out = out.cpu()\n",
    "        \n",
    "        out_all_val[idx,:] = out\n",
    "        l_all_val[idx] = l\n",
    "        c_all_val[idx] = c\n",
    "        l_cont_all_val[idx] = l_cont\n",
    "\n",
    "h = nn.Sigmoid()(out_all_val)\n",
    "S = torch.cumprod(1-h,dim = -1)\n",
    "risk = -S.sum(dim=1) ## TODO why is it not 1-S ???\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import c_index\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "c_ind = c_index(out_all_val,c_all_val,l_all_val)\n",
    "plt.title(f\"risk distribution with c_index= {round(c_ind,3)}\")\n",
    "plt.hist(risk.numpy(),bins = 100)\n",
    "plt.show()\n",
    "test_ds.gen_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4 \n",
    "eps=0.1\n",
    "threshold = np.linspace(risk.min()+eps,risk.max()-eps,num=12)\n",
    "fig,ax1 = plt.subplots(rows,len(threshold)//rows,figsize=(24,8))\n",
    "fig.tight_layout()\n",
    "for i in range(len(threshold)):\n",
    "    risk_bool = risk>threshold[i]\n",
    "    event1 = np.asarray(1-c_all_val[risk_bool]).astype(bool)\n",
    "    time_exit1 = np.asarray(l_cont_all_val[risk_bool])\n",
    "    x1, y1 = kaplan_meier_estimator(event1, time_exit1)\n",
    "\n",
    "    event2 = np.asarray(1-c_all_val[~risk_bool]).astype(bool)\n",
    "    time_exit2 = np.asarray(l_cont_all_val[~risk_bool])\n",
    "    x2, y2 = kaplan_meier_estimator(event2, time_exit2)\n",
    "    \n",
    "    x_full, y_full = kaplan_meier_estimator(event, time_exit)\n",
    "\n",
    "    ax1.flatten()[i].set_title(f\"KM for threshold {round(threshold[i],3)}\")\n",
    "    ax1.flatten()[i].step(x1, y1, where=\"post\",label=\"high risk\")\n",
    "    ax1.flatten()[i].step(x2, y2, where=\"post\",label=\"low risk\")\n",
    "    ax1.flatten()[i].step(x_full, y_full, where=\"post\",label=\"full KM\")\n",
    "    \n",
    "    ax1.flatten()[i].set_ylim(0, 1.1)\n",
    "    ax1.flatten()[i].grid()\n",
    "    ax1.flatten()[i].legend()\n",
    "\n",
    "\n",
    "#plt.clf\n",
    "#plt.title(f\"KM for threshold {round(threshold[i])}\")\n",
    "#plt.step(x1, y1, where=\"post\",label=\"high risk\")\n",
    "#plt.step(x2, y2, where=\"post\",label=\"low risk\")\n",
    "#plt.ylim(0, 1.1)\n",
    "#plt.grid()\n",
    "#plt.legend()\n",
    "#plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Scatter risk vs survival month\")\n",
    "plt.scatter(risk[c_all_val.type(torch.bool)],l_cont_all_val[c_all_val.type(torch.bool)],s = 4,label=\"censored\")\n",
    "plt.scatter(risk[~(c_all_val.type(torch.bool))],l_cont_all_val[~(c_all_val.type(torch.bool))],s = 4,label=\"uncensored\")\n",
    "plt.xlabel(\"Risk score\")\n",
    "plt.ylabel(\"Survival month\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.title(f\"risk distribution\")\n",
    "plt.hist(risk[c_all_val.type(torch.bool)].numpy(),bins = 30,alpha=0.5,label=\"a\")\n",
    "plt.hist(risk[~(c_all_val.type(torch.bool))].numpy(),bins = 30,alpha=0.5,label=\"b\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt \n",
    "wandb.init(project=\"MultiModal\",entity=\"tobias-seibel\",name=\"tests\")\n",
    "out_all_val = out_all_val\n",
    "n_thresholds = 4\n",
    "nbins = 30\n",
    "c_all_val = c_all_val\n",
    "l_cont_all_val = l_cont_all_val \n",
    "\n",
    "#risk\n",
    "def get_risk(out):\n",
    "    h = nn.Sigmoid()(out)\n",
    "    S = torch.cumprod(1-h,dim = -1)\n",
    "    risk = -S.sum(dim=1)\n",
    "    return risk\n",
    "risk_all = get_risk(out_all_val)\n",
    "\n",
    "#thresholds\n",
    "min,max = risk_all.min(),risk_all.max()\n",
    "thresholds = torch.linspace(min,max,n_thresholds+2)[1:-1]\n",
    "\n",
    "#hist \n",
    "censored = c_all_val.type(torch.bool)\n",
    "uncensored = ~c_all_val.type(torch.bool)\n",
    "\n",
    "x_c1 = torch.histc(risk_all[censored],bins=nbins,min = min , max =max ) \n",
    "x1_label = torch.ones_like(x_c1)\n",
    "x_c2 = torch.histc(risk_all[uncensored],bins=nbins,min = min , max =max ) \n",
    "x2_label = torch.zeros_like(x_c2)\n",
    "x_all = torch.stack((x_c1,x_c2)).T\n",
    "\n",
    "\n",
    "#table = wandb.Table(data = x_all.tolist(),columns=[\"censored\",\"uncensored\"])\n",
    "table = wandb.plot.line_series(\n",
    "          xs =np.linspace(min,max,nbins),\n",
    "          ys=[x_c1,x_c2],\n",
    "          keys=[\"censored\", \"uncensored\"],\n",
    "          title=f\"Histogramm\",\n",
    "          xname=\"risk\")\n",
    "wandb.log({\"custom\":table})\n",
    "#KM\n",
    "\n",
    "#stepfunction\n",
    "def stepfunc(x,y,eps=1e-4):\n",
    "    x = np.stack((x-eps,x),axis=1).flatten()\n",
    "    y = np.stack((y,y),axis=1).flatten()\n",
    "    return x[1:].copy(),y[:-1].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for idx,threshold in enumerate(thresholds.tolist()): #thresholds\n",
    "    x_full, y_full = kaplan_meier_estimator(uncensored.numpy(), l_cont_all_val)\n",
    "\n",
    "    xlow, ylow = kaplan_meier_estimator(uncensored[risk>threshold].numpy(),\n",
    "                                l_cont_all_val[risk>threshold].numpy())\n",
    "\n",
    "    xhigh, yhigh = kaplan_meier_estimator(uncensored[risk<=threshold].numpy(),\n",
    "                                l_cont_all_val[risk<=threshold].numpy())\n",
    "    \n",
    "    xfull, yfull = stepfunc(x_full, y_full)\n",
    "    xlow, ylow =stepfunc(xlow, ylow)\n",
    "    xhigh, yhigh =stepfunc(xhigh, yhigh)\n",
    "\n",
    "    #x,y1,y2,y3 = KM_table(xlow,ylow,xhigh,yhigh,xfull,yfull,)\n",
    "    lineseries = wandb.plot.line_series(\n",
    "          xs=[xlow,xhigh,xfull],\n",
    "          ys=[ylow,yhigh,yfull],\n",
    "          keys=[\"lowrisk\", \"highrisk\",\"fullrisk\"],\n",
    "          title=f\"KM Stratification at risk={str(round(threshold,2)).replace('.',',')}\",\n",
    "          xname=\"time\")\n",
    "    \n",
    "    wandb.log({f\"KM_{idx}\" :lineseries})\n",
    "    \n",
    "\n",
    "# Log the table to wandb\n",
    "wandb.log({\"line_series\": table})\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def KM_table(xlow,ylow,xhigh,yhigh,xfull,yfull,):\n",
    "    ### get KM curve on same x values... not needed anymore FML\n",
    "    low_all = np.stack((xlow,ylow,np.empty_like(ylow)*np.nan,np.empty_like(ylow)*np.nan),1)\n",
    "    high_all = np.stack((xhigh,np.empty_like(yhigh)*np.nan,yhigh,np.empty_like(yhigh)*np.nan),1)\n",
    "    full_all = np.stack((xfull,np.empty_like(yfull)*np.nan,np.empty_like(yfull)*np.nan, yfull),1)\n",
    "    all = np.concatenate((low_all,high_all,full_all),axis=0)\n",
    "\n",
    "    all = all[all[:, 0].argsort()] # sort by first element \n",
    "\n",
    "    for i in range(1,len(all)):\n",
    "        a = all[i-1,1:] # =:prev-line\n",
    "        b = all[i,1:]  # =:line\n",
    "        all[i,1:] = np.where(np.isnan(b),a,b)\n",
    "        if all[i,0]==all[i-1,0]: #if on same x give info to line, set x of prev-line to nan\n",
    "            all[i-1,0] = np.nan\n",
    "            \n",
    "    all = all[all[:, 0].argsort()]\n",
    "    all = all[~np.isnan(all[:,0])]\n",
    "    return all[:,0],all[:,1],all[:,2],all[:,3]  # x, y_low,y_high,y_full\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final FUnctions for Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def KM_wandb(run,out,c,event_cond,n_thresholds = 4,nbins = 30):\n",
    "    print(\"Start Logging KM-Estimators\")\n",
    "    risk = get_risk(out)\n",
    "    \n",
    "    #thresholds\n",
    "    min,max = risk.min(),risk.max()\n",
    "    thresholds = np.linspace(min,max,n_thresholds+2)[1:-1]\n",
    "    \n",
    "    #hist\n",
    "    censored = c.type(torch.bool)\n",
    "    uncensored = ~c.type(torch.bool)\n",
    "    \n",
    "    hist_censored = torch.histc(risk[censored],bins=nbins,min = min , max =max ) \n",
    "    hist_uncensored = torch.histc(risk[uncensored],bins=nbins,min = min , max =max ) \n",
    "    \n",
    "    table = run.plot.line_series(\n",
    "          xs =np.linspace(min,max,nbins),\n",
    "          ys=[hist_censored,hist_uncensored],\n",
    "          keys=[\"censored\", \"uncensored\"],\n",
    "          title=f\"Histogramm\",\n",
    "          xname=\"risk\")\n",
    "    run.log({\"risk_histogramm\":table})\n",
    "    \n",
    "    #KaplanMeier Plots\n",
    "    x_full, y_full = kaplan_meier_estimator(uncensored.numpy(), event_cond)\n",
    "    xfull, yfull = stepfunc(x_full, y_full)\n",
    "    for idx,threshold in enumerate(thresholds): \n",
    "        xlow, ylow = kaplan_meier_estimator(uncensored[risk>threshold].numpy(),\n",
    "                                    event_cond[risk>threshold])\n",
    "\n",
    "        xhigh, yhigh = kaplan_meier_estimator(uncensored[risk<=threshold].numpy(),\n",
    "                                    event_cond[risk<=threshold])\n",
    "        \n",
    "        xlow, ylow =stepfunc(xlow, ylow)\n",
    "        xhigh, yhigh =stepfunc(xhigh, yhigh)\n",
    "\n",
    "        \n",
    "        lineseries = run.plot.line_series(\n",
    "            xs=[xlow,xhigh,xfull],\n",
    "            ys=[ylow,yhigh,yfull],\n",
    "            keys=[\"lowrisk\", \"highrisk\",\"fullrisk\"],\n",
    "            title=f\"KM Stratification at risk={str(round(threshold,2)).replace('.',',')}\",\n",
    "            xname=\"time\")\n",
    "        \n",
    "        run.log({f\"KM_{idx}\" :lineseries})\n",
    "    print(\"Finished logging KM-Estimators\")\n",
    "    \n",
    "def get_risk(out):\n",
    "    h = nn.Sigmoid()(out)\n",
    "    S = torch.cumprod(1-h,dim = -1)\n",
    "    risk = -S.sum(dim=1)\n",
    "    return risk\n",
    "\n",
    "def stepfunc(x,y,eps=1e-4):\n",
    "    x = np.stack((x-eps,x),axis=1).flatten()\n",
    "    y = np.stack((y,y),axis=1).flatten()\n",
    "    return x[1:],y[:-1]\n",
    "\n",
    "def do_table(x,y,label):\n",
    "    return [[x[i],y[i],label] for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import matplotlib.pyplot as plt \n",
    "wandb.init(project=\"MultiModal\",entity=\"tobias-seibel\",name=\"tests\")\n",
    "out_all_val = out_all_val\n",
    "n_thresholds = 4\n",
    "nbins = 30\n",
    "c_all_val = c_all_val\n",
    "l_cont_all_val = l_cont_all_val \n",
    "\n",
    "#risk\n",
    "def get_risk(out):\n",
    "    h = nn.Sigmoid()(out)\n",
    "    S = torch.cumprod(1-h,dim = -1)\n",
    "    risk = -S.sum(dim=1)\n",
    "    return risk\n",
    "risk_all = get_risk(out_all_val)\n",
    "\n",
    "#thresholds\n",
    "min,max = risk_all.min(),risk_all.max()\n",
    "thresholds = torch.linspace(min,max,n_thresholds+2)[1:-1]\n",
    "\n",
    "#hist \n",
    "censored = c_all_val.type(torch.bool)\n",
    "uncensored = ~c_all_val.type(torch.bool)\n",
    "\n",
    "x_c1 = torch.histc(risk_all[censored],bins=nbins,min = min , max =max ).numpy()\n",
    "x1_label = np.chararray(np.shape(x_c1))\n",
    "x1_label[:]=\"censored\"\n",
    "x_c2 = torch.histc(risk_all[uncensored],bins=nbins,min = min , max =max ).numpy() \n",
    "x2_label = np.chararray(np.shape(x_c2))\n",
    "x2_label[:]=\"uncensored\"\n",
    "\n",
    "x1 = np.stack((np.linspace(min,max,nbins),x_c1,x1_label))\n",
    "x2 = np.stack((np.linspace(min,max,nbins),x_c2,x2_label))\n",
    "np.concatenate((x1,x2),dim=1).T\n",
    "\n",
    "table = wandb.Table(\n",
    "          data = np.concatenate((x1,x2),dim=1).T,\n",
    "          columns=[\"bins\", \"risk\",\"category\"],\n",
    "          \n",
    "          \n",
    "\n",
    "          )\n",
    "wandb.log({\"custom\":table})\n",
    "#KM\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better version for custom plots! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.linspace(min,max,nbins)\n",
    "x_c1 = torch.histc(risk_all[censored],bins=nbins,min = min , max =max ).numpy()\n",
    "x_c2 = torch.histc(risk_all[uncensored],bins=nbins,min = min , max =max ).numpy() \n",
    "\n",
    "table_hist = wandb.Table(\n",
    "          data = do_table(x,x_c1,\"censored\")+do_table(x,x_c2,\"uncensored\"),\n",
    "          columns=[\"risk\", \"count\",\"category\"],\n",
    "          )\n",
    "\n",
    "fields_hist = {\"x\":\"risk\",\"y\":\"count\",\"groupKeys\":\"category\",\"title\":\"Risk Distribution\"}\n",
    "custom_histogram = wandb.plot_table(vega_spec_name=\"tobias-seibel/risk_distribution\",\n",
    "              data_table=table_hist,\n",
    "              fields = fields_hist )\n",
    "              \n",
    "              \n",
    "event_cond = l_cont_all_val\n",
    "threshold = -2\n",
    "xfull, yfull = kaplan_meier_estimator(uncensored.numpy(), event_cond)\n",
    "\n",
    "xlow, ylow = kaplan_meier_estimator(uncensored[risk>threshold].numpy(),\n",
    "                                    event_cond[risk>threshold])\n",
    "\n",
    "xhigh, yhigh = kaplan_meier_estimator(uncensored[risk<=threshold].numpy(),\n",
    "                                    event_cond[risk<=threshold])\n",
    "        \n",
    "\n",
    "\n",
    "table_KM = wandb.Table(\n",
    "          data = do_table(xlow,ylow,\"low risk group\")+do_table(xhigh,yhigh,\"risk high group\")+do_table(xfull,yfull,\"total group\"),\n",
    "          columns=[\"time\",\"Survival Probability\",\"Group\"],)\n",
    "\n",
    "field_KM = {\"x\":\"time\",\"y\":\"Survival Probability\",\"groupKeys\":\"Group\"}\n",
    "custom_KM = wandb.plot_table(vega_spec_name=\"tobias-seibel/kaplanmeier\",\n",
    "              data_table=table_KM,\n",
    "              fields = field_KM, \n",
    "              string_fields={\"title\":f\"KM Risk Stratification at {round(threshold,2)}\"},\n",
    "              )\n",
    "\n",
    "wandb.init()\n",
    "\n",
    "idx = 1 \n",
    "wandb.log({\"Risk Distribution\":custom_histogram,\"KM{idx}\":custom_KM}) #tobias-seibel/risk_distribution # tobias-seibel/kaplanmeier\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KM_wandb(run,out,c,event_cond,n_thresholds = 4,nbins = 30):\n",
    "    print(\"Start Logging KM-Estimators\")\n",
    "    risk = get_risk(out)\n",
    "    \n",
    "    #thresholds\n",
    "    min,max = risk.min(),risk.max()\n",
    "    thresholds = np.linspace(min,max,n_thresholds+2)[1:-1]\n",
    "    \n",
    "    #hist\n",
    "    censored = c.type(torch.bool)\n",
    "    uncensored = ~c.type(torch.bool)\n",
    "    \n",
    "    ###wandb histogram\n",
    "    x = np.linspace(min,max,nbins)\n",
    "    x_c1 = torch.histc(risk_all[censored],bins=nbins,min = min , max =max ).numpy()\n",
    "    x_c2 = torch.histc(risk_all[uncensored],bins=nbins,min = min , max =max ).numpy() \n",
    "\n",
    "    table_hist = wandb.Table(\n",
    "            data = do_table(x,x_c1,\"censored\")+do_table(x,x_c2,\"uncensored\"),\n",
    "            columns=[\"risk\", \"count\",\"category\"],\n",
    "            )\n",
    "\n",
    "    fields_hist = {\"x\":\"risk\",\"y\":\"count\",\"groupKeys\":\"category\",\"title\":\"Risk Distribution\"}\n",
    "    custom_histogram = wandb.plot_table(vega_spec_name=\"tobias-seibel/risk_distribution\",\n",
    "                data_table=table_hist,\n",
    "                fields = fields_hist )\n",
    "    \n",
    "    wandb.log({\"Risk Distribution\":custom_histogram})\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #KaplanMeier Plots\n",
    "    xfull, yfull = kaplan_meier_estimator(uncensored.numpy(), event_cond)\n",
    "    \n",
    "    for idx,threshold in enumerate(thresholds): \n",
    "        xlow, ylow = kaplan_meier_estimator(uncensored[risk>threshold].numpy(),\n",
    "                                    event_cond[risk>threshold])\n",
    "\n",
    "        xhigh, yhigh = kaplan_meier_estimator(uncensored[risk<=threshold].numpy(),\n",
    "                                    event_cond[risk<=threshold])\n",
    "        \n",
    "        \n",
    "        table_KM = wandb.Table(data = do_table(xlow,ylow,\"low risk group\")+do_table(xhigh,yhigh,\"risk high group\")+do_table(xfull,yfull,\"total group\"),\n",
    "                        columns=[\"time\",\"Survival Probability\",\"Group\"],)\n",
    "\n",
    "        field_KM = {\"x\":\"time\",\"y\":\"Survival Probability\",\"groupKeys\":\"Group\"}\n",
    "        custom_KM = wandb.plot_table(vega_spec_name=\"tobias-seibel/kaplanmeier\",\n",
    "                    data_table=table_KM,\n",
    "                    fields = field_KM, \n",
    "                    string_fields={\"title\":f\"KM Risk Stratification at {round(threshold,2)}\"},\n",
    "                    )\n",
    "        run.log({f\"KM_{idx}\" :custom_KM})\n",
    "    print(\"Finished logging KM-Estimators\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init()\n",
    "KM_wandb(run,out_all_val,c_all_val,l_cont_all_val,n_thresholds = 4,nbins = 30)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seibel/.local/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from utils import *\n",
    "class Classifier_Head(nn.Module):\n",
    "    def __init__(self,outsize,d_hidden=256,t_bins=4):\n",
    "        super(Classifier_Head,self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(outsize,d_hidden)\n",
    "        torch.nn.init.kaiming_normal_(self.linear1.weight)\n",
    "        self.activ1 = nn.ReLU()\n",
    "        self.linear2  = nn.Linear(d_hidden,d_hidden)\n",
    "        torch.nn.init.kaiming_normal_(self.linear2.weight)\n",
    "        self.activ2 = nn.ReLU()\n",
    "        self.fc = nn.Linear(d_hidden,t_bins) # TODO test add layer\n",
    "    def forward(self,x):\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.activ1(self.linear1(x))\n",
    "        x = self.activ2(self.linear2(x))\n",
    "        return self.fc(x)\n",
    "    \n",
    "    \n",
    "class TransformerMil_Survival(nn.Module):\n",
    "  def __init__(self,d_seq,d_transformer,bins):\n",
    "    super(TransformerMil_Survival,self).__init__()\n",
    "    d_out = d_hidden = 256\n",
    "    self.lin_embedder1 = nn.Linear(d_seq,d_transformer,dropout)\n",
    "    #self.Encoder = torch.nn.TransformerEncoder(nn.TransformerEncoderLayer(d_transformer,\n",
    "    #                                                                 nhead=2,dropout=0.1,activation=nn.GELU(),batch_first=True)\n",
    "    #                                      ,num_layers=2)\n",
    "    self.Encoder=nn.TransformerEncoderLayer(d_transformer,nhead=2,dropout=dropout,activation=nn.GELU(),batch_first=True)\n",
    "    self.lin_embedder2 = nn.Linear(d_transformer,d_out)\n",
    "    self.Classifier_Head = Classifier_Head(outsize = d_out,d_hidden=d_hidden,t_bins=bins)\n",
    "  def forward(self,x):\n",
    "    \n",
    "    x = self.lin_embedder1(x)\n",
    "    x = self.Encoder(x)\n",
    "    out = x.mean(dim=-2)\n",
    "    out = self.lin_embedder2(out)\n",
    "    return self.Classifier_Head(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "bins = 4\n",
    "model = TransformerMil_Survival(d_seq=2048,d_transformer=1024,bins=bins)\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = Survival_Loss(0.25) \n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.00003,betas=[0.9,0.999],weight_decay=1e-5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 10.92 GiB total capacity; 10.02 GiB already allocated; 60.69 MiB free; 10.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d651238b0ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m## Groundtruth boolean information wether the the patient is censored  c = 1 or uncensored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-84a16b60053f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_embedder1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin_embedder2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask)\u001b[0m\n\u001b[1;32m    348\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                            need_weights=False)[0]\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m   1008\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m   1011\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   5099\u001b[0m     \u001b[0;31m# (deep breath) calculate attention and out projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5100\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5101\u001b[0;31m     \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_scaled_dot_product_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5102\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5103\u001b[0m     \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_scaled_dot_product_attention\u001b[0;34m(q, k, v, attn_mask, dropout_p)\u001b[0m\n\u001b[1;32m   4847\u001b[0m     \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4849\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4850\u001b[0m     \u001b[0;31m# (B, Nt, Ns) x (B, Ns, E) -> (B, Nt, E)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4851\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/seibel/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dropout probability has to be between 0 and 1, \"\u001b[0m \u001b[0;34m\"but got {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 10.92 GiB total capacity; 10.02 GiB already allocated; 60.69 MiB free; 10.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "nseq = 15000\n",
    "B=1\n",
    "dims = 2048\n",
    "\n",
    "x = torch.rand(size=(B,nseq,dims)).to(device)\n",
    "l = torch.randint(low=0, high=bins, size=(B,)).to(device)\n",
    "c = torch.randint(low=0, high=2, size=(B,)).to(device)  ## Groundtruth boolean information wether the the patient is censored  c = 1 or uncensored  \n",
    "\n",
    "out = model(x)\n",
    "loss = criterion(out,c,l)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
